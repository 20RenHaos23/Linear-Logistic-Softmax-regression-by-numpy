#Linear regressionç·´ç¿’

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import LogNorm

#è®€å–è³‡æ–™
x , y = [] ,[]
with open('food_truck_data.txt') as A:
    for eachline in A:
        s = eachline.split(',')
        x.append(float(s[0]))
        y.append(float(s[1]))  

    
#å°‡è®€å–çš„è³‡æ–™ç•«å‡ºä¾†
fig, ax = plt.subplots()   
ax.scatter(x, y, marker="x", c="red")
plt.title("Food Truck Dataset", fontsize=16)
plt.xlabel("City Population in 10,000s", fontsize=14)
plt.ylabel("Food Truck Profit in 10,000s", fontsize=14)
plt.axis([4, 25, -5, 25])#è¨­ç½®åœ–å½¢çš„åæ¨™è»¸ç¯„åœï¼Œxè»¸çš„ç¯„åœæ˜¯4åˆ°25ï¼Œyè»¸çš„ç¯„åœæ˜¯-5åˆ°25ã€‚
plt.show()



#ä½¿ç”¨æ­£è¦æ–¹ç¨‹æ³•æ±‚è§£ç·šæ€§å›æ­¸æ¨¡å‹çš„åƒæ•¸
data = np.loadtxt('food_truck_data.txt', delimiter=",") # dataæ˜¯m*2çŸ©é˜µ
train_x = data[:, 0]    # åŸå¸‚äººå£ï¼Œ  m*1çŸ©é˜µ 
train_y = data[:, 1]    # é¤è½¦åˆ©æ¶¦ï¼Œ  m*1çŸ©é˜µ



#æ±‚è§£ç·šæ€§è¿´æ­¸çš„æ¢¯åº¦ä¸‹é™æ³•æ¼”ç®—æ³•çš„ç¨‹å¼ç¢¼
X = train_x
w,b = 0.,0.
dw = np.mean((w*X+b-y)*X)
db = np.mean((w*X+b-y))


def gradient_descent(x,y,w,b,alpha=0.01, iterations = 100,epsilon = 1e-9):    
    history=[]
    for i in range(iterations):
        dw = np.mean((w*x+b-y)*x)
        db = np.mean((w*x+b-y))       
        if abs(dw) < epsilon and abs(db) < epsilon:
           break;
     
        #æ›´æ–°w: w = w - alpha * gradient
        w -= alpha*dw 
        b -= alpha*db 
        history.append([w,b])  
       
    return history

alpha = 0.02
iterations=1000
history = gradient_descent(X,y,w,b,alpha,iterations)
print()
print('ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ç·šæ€§å›æ­¸æ¨¡å‹çš„åƒæ•¸')
print('w : {},b : {}'.format(history[-1][0],history[-1][1]))

'''
----------------------------------------------------------------------------------------
'''


def draw_line(plt,w,b,x,linewidth =2):
    m=len(x)
    f = [0]*m
    for i in range(m): 
       f[i] = b+w*x[i]
    plt.plot(x, f, linewidth) 
  
fig, ax = plt.subplots() 
plt.scatter(X, y, marker="x", c="red")
plt.title("Food Truck Dataset", fontsize=16)
plt.xlabel("City Population in 10,000s", fontsize=14)
plt.ylabel("Food Truck Profit in 10,000s", fontsize=14)
plt.axis([4, 25, -5, 25])
w,b = history[-1]
draw_line(plt,w,b,X,6)
plt.show()


def loss(x,y,w,b):
    
    return np.mean((x*w+b-y)**2)/2 #å†é™¤2 ç‚ºæ ¹æ“š3-8é å¯«çš„
    '''
    
    #å¦ä¸€ç¨®æ–¹å¼
    m = len(y)
    cost = 0   
    for i in range(m):  
        f =  x[i]*w+b
        cost += (f-y[i])**2
    cost /=(2*m) #é™¤2m ç‚ºæ ¹æ“š3-8é å¯«çš„
    return cost
    '''

#print(loss(X,y,1,-3))

costs = [loss(X,y,w,b) for w,b in history]
plt.axis([0, len(costs), 4, 6])#è¨­ç½®åœ–å½¢çš„åæ¨™è»¸ç¯„åœï¼Œxè»¸çš„ç¯„åœæ˜¯0åˆ°len(costs)ï¼Œyè»¸çš„ç¯„åœæ˜¯4åˆ°6ã€‚
plt.plot(costs)
plt.grid()
plt.show()

def plot_history(x,y,history,figsize=(20,10)):
    w= [ e[0] for e in history]
    b= [ e[1] for e in history]
    #ç”Ÿæˆ w å’Œ b çš„ç¶²æ ¼
    xmin,xmax, xstep = min(w)-0.2,max(w)+0.2, .2 # 0.26 1.50 0.2
    ymin, ymax, ystep = min(b)-0.2,max(b)+0.2, .2 # -3.98 0.31 0.2
    ws,bs = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))
    
    #è¨ˆç®—æå¤±å‡½æ•¸
    zs = np.array([loss(x, y, w,b)    for w,b in zip(np.ravel(ws), np.ravel(bs))]) #é«˜
    z = zs.reshape(ws.shape)
    
    #3Dç¹ªåœ–çš„åŸºæœ¬è¨­ç½®
    fig = plt.figure(figsize=figsize)
    #ax = fig.add_subplot(111, projection='3d')
    ax = plt.axes(projection='3d')
    
    ax.set_xlabel('w', labelpad=30, fontsize=24, fontweight='bold')
    ax.set_ylabel('b', labelpad=30, fontsize=24, fontweight='bold')
    ax.set_zlabel('L(w,b)', labelpad=30, fontsize=24, fontweight='bold')
    
    #ç¹ªè£½æå¤±å‡½æ•¸çš„3Dè¡¨é¢åœ–
    ax.plot_surface(ws, bs, z, rstride=1, cstride=1, color='b', alpha=0.2)
    
    #ç¹ªè£½å„ªåŒ–éç¨‹çš„èµ·é»å’Œçµ‚é»
    w_sart,b_start,w_end,b_end = history[0][0], history[0][1],history[-1][0], history[-1][1]
    ax.plot([w_sart],[b_start], [loss(x,y,w_sart,b_start)] , markerfacecolor='b', markeredgecolor='b', marker='o', markersize=7)
    ax.plot([w_end],[b_end], [loss(x,y,w_end,b_end)] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)
    
    #æç¹ªæ¢¯åº¦ä¸‹é™çš„è»Œè·¡
    z2 =  [loss(x,y,w,b) for w,b in history]
    ax.plot(w, b, z2  , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2) #é»ä¹‹é–“æœƒè‡ªå‹•ç”¨ç·šé€£æ¥èµ·ä¾†
    ax.plot(w, b,  0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2) #ç•«æˆå¹³é¢çš„æ¨£å­ zè»¸çš†ç‚º0
    
    fig.suptitle("L(w,b)", fontsize=24, fontweight='bold')
    plt.show()
    return ws,bs,z
    
ws,bs,z = plot_history(X,y,history)


#ç¹ªè£½ç­‰é«˜ç·šåœ–:
plt.figure()
plt.contour(bs,ws,z,levels=np.logspace(-5, 5, 100), norm=LogNorm(), cmap=plt.cm.jet) 

w= [ e[0] for e in history]
b= [ e[1] for e in history]
plt.plot(b,w)
plt.xlabel("b")
plt.ylabel("w")
title = str.format("iteration={0}, alpha={1}, b={2:.3f}, w={3:.3f}", iterations, alpha, b[-1], w[-1])
plt.title(title)
plt.show()

#ä½¿ç”¨ä¸åŒlearning rateï¼Œä¸¦ç•«å‡ºåœ–
plt.figure()
num_iters = 1200
learning_rates = [0.01, 0.015, 0.02]
for lr in learning_rates:
    w,b=0,0 
    history = gradient_descent(X, y,w, b,lr, num_iters)
    cost_history = [loss(X,y,w,b) for w,b in history]
    plt.plot(cost_history, linewidth=2)
plt.title("Gradient descent with different learning rates", fontsize=16)
plt.xlabel("number of iterations", fontsize=14)
plt.ylabel("cost", fontsize=14)
plt.legend(list(map(str, learning_rates)))
plt.axis([0, num_iters, 4, 6])
plt.grid()
plt.show()


#æ¢¯åº¦é©—è­‰
df_approx = lambda x,y,w,b,eps: ( (loss(x,y,w+eps,b)-loss(x,y,w-eps,b) )/(2*eps),  (loss(x,y,w,b+eps)-loss(x,y,w,b-eps) )/(2*eps) )
#åœ¨ä»»æ„ä¸€ä¸ªç‚¹å¦‚ (ğ‘¤,ğ‘)=(1.0,âˆ’2.0) æ¯”è¾ƒåˆ†æå’Œæ•°å€¼æ¢¯åº¦ã€‚
w = 1.0
b = -2.
eps = 1e-8
dw = np.mean((w*X+b-y)*X)
db = np.mean((w*X+b-y))                                   
grad = np.array([dw,db])
grad_approx = np.array(df_approx(X,y,w,b,eps))
print()
print('æ¢¯åº¦é©—è­‰')
print(grad)
print(grad_approx)
print(abs(grad-grad_approx))



#ç”¨æ±‚å¾—çš„w bæ±‚Xç§æ ·æœ¬çš„é¢„æµ‹å€¼
m=len(X)
predictions = [0]*m   
for i in range(m): 
    predictions[i] =  X[i]*1.1822480052540145 + -3.7884192615511796

plt.scatter(X, y, marker="x", c="red")
plt.scatter(X, predictions, marker="o", c="blue") 
#plt.plot(X, predictions, linewidth=2)  # plot the hypothesis on top of the training data
plt.show()